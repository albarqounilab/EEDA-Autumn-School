{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression (~ 100 min)\n",
    "\n",
    "![title](1.png)\n",
    "\n",
    "In this notebook we will explore and work on the Iris dataset (that deals with different types of the iris flower). The Iris flower data set or Fisher's Iris data set is a multivariate data set used and made famous by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. Further information for which can be found at https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "Image Source: Unwin, Antony, and Kim Kleinman. \"The iris data set: In search of the source of virginica.\" Significance 18.6 (2021): 26-29.\n",
    "First we have to import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdhvkD9aWBQf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings \n",
    "#Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo (15 min):\n",
    "\n",
    "What is each library helpful for? (You can use Google to familiarise yourself with the libraries)\n",
    "Which of the lines above imports a package? Which imports a function?\n",
    "\n",
    "#Loading the Data (10 min)\n",
    "\n",
    "Use the load_iris function we have imported in the previous step to load the dataset (note that the function returns the dataset, which has to be saved into a variable).\n",
    "\n",
    "Create a pandas dataframe object containing the features (the iris['data']) and the target values (iris[target]). Please note that column names correspond to feature names (iris['feature_names]), and an additional column is necessary for the target value.\n",
    "\n",
    "Familiarise yourself with the dataset. How many different varieties of iris are included in this dataset?\n",
    "You can use pd.head() to print the first 5 records of the dataset. \n",
    "You can use column_example.unique() to find all the unique elements in the example column of the pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11-KNhRsW2Mx"
   },
   "source": [
    "load the data and know how many classes are there (target column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "MlPw3O_XWbvU",
    "outputId": "0855fe6a-5b60-42b9-c9fa-770d8761fa31"
   },
   "outputs": [],
   "source": [
    "#ToDo: Load the dataset\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Explain the parameters of the initialising function\n",
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target']) \n",
    "\n",
    "#ToDo: Print the different possible target values \n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Print the first 5 records of the dataset\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (15 min)\n",
    "\n",
    "A table of numbers is not useful for interpreting dependencies between variables. So let's plot the data we have and see which features correlate with each other.\n",
    "\n",
    "We will use 2 different types of plots: Scatter plots and histograms. What is the difference? Familiarize ureself with these kind of plots in the matplot library.\n",
    "As we have 14 different features and we want to plot the correlation between each feature, we need a total of 196 plots. Fill in the missing parts in the code below. You can use axs[COLUMN_INDEX, ROW_INDEX] to select a subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yJDiPoOOXRyt",
    "outputId": "4205c24f-c2b4-4d97-a4ae-3f2fcf0736ee"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(dataset.columns), len(dataset.columns), figsize=(25, 25))\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    for j in range(len(dataset.columns)):\n",
    "        if i == j:\n",
    "            # We plot the histogram of column i on axis i, j \n",
    "            axs[i,j].hist(dataset[dataset.columns[i]])\n",
    "        else:\n",
    "            #ToDo: scatter plot the data points with column i as x and column j as y,\n",
    "            #with the color of the dot defined by target value \n",
    "            #Insert your own code here\n",
    "        axs[0,j].set_title(dataset.columns[j])\n",
    "    axs[i,0].set(ylabel=dataset.columns[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our task analysis and future prediction it is also important to calculate which features are correlated with the target value and which features are correlated between themselves. We can visualise it for ourselves in a helpful way as a correlation matrix.\n",
    "\n",
    "The correlation matrix (for more details https://www.displayr.com/what-is-a-correlation-matrix/#:~:text=A%20correlation%20matrix%20is%20a,a%20diagnostic%20for%20advanced%20analyses) \"is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\"\n",
    "\n",
    "In a correlation matrix we represent the correlation as a number (correlation coefficient)\n",
    "\n",
    "You can use a_pandas_dataframe.corr() to compute the correlation matrix on your dataframe a_pandas_dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "id": "v-tHzbbwbhVV",
    "outputId": "638cb6b9-fc10-4d40-d8a2-7edaf8800cd6"
   },
   "outputs": [],
   "source": [
    "#ToDo compute the correlation matrix (into the variable correlation_matrix)\n",
    "#Insert your own code here\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "#ToDo explain what the negative and positive values represent\n",
    "#Optional : change the color scheme (see documentation of the function for help)\n",
    "sns.heatmap(data=correlation_matrix, annot=True, ax=ax) # annot = True to print the values inside the square\n",
    "plt.show()\n",
    "dataset.boxplot(by=\"target\", figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (15 min)\n",
    "\n",
    "In machine learning, the performance of any model is usually checked using a test set, which is a set of data not used in the training process. This makes the evaluation more trustworthy.\n",
    "\n",
    "We will split the dataset randomly into train and test set, with the test set including 25% of the data.\n",
    "\n",
    "Note that we have imported the function train_test_split from the sklearn module, submodel model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EnteaJU89NJ"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X = dataset.iloc[:, [0,1,2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "#ToDo split the dataset into train and test of 25% (into X_train, X_test, y_train, y_test) with random_state=0\n",
    "#Insert your own code here\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the classifier, we need to create and initialise the classifier object (in this case, LogisticRegression object). \n",
    "\n",
    "You can use the classifier.fit() function to fit the classifier on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "I_w1a8Rk9k2F",
    "outputId": "b6526abe-3145-4462-dd9f-b4bb50e8d844"
   },
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "#Initialising the classifier\n",
    "classifier = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "#ToDo: fit the classifier to the train set\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the classifier (15 min)\n",
    "\n",
    "After fitting the model, we need to evaluate the performance of the trained model to decide if the complexity of the method or amount of training data is sufficient.\n",
    "\n",
    "The process has several steps:\n",
    "1. Use the trained model to predict the target values of the test set.\n",
    "2. Predict the probabilities for each target class on the test set using the trained model.\n",
    "3. Compare the probabilities returned to the correct result\n",
    "4. Create a confusion matrix\n",
    "\n",
    "We can use classifer.predict() to get the predictions as one number.\n",
    "We can use classifier.predict_proba() to get get the prediction in the form of probabilities for each target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC71-n2XBuSY"
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Predict probabilities (into probs_y variable)\n",
    "#Insert your own code here\n",
    "\n",
    "probs_y = np.round(probs_y, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYHtJfesB7vm",
    "outputId": "4a7a191a-6a95-47a2-91c5-c79c94eef96f"
   },
   "outputs": [],
   "source": [
    "#We plot a table with target results, predicted class and probabilities for each class for every data point\n",
    "res = \"{:<10} | {:<10} | {:<10} | {:<13} | {:<5}\".format(\"y_test\", \"y_pred\", \"Setosa(%)\", \"versicolor(%)\", \"virginica(%)\\n\")\n",
    "res += \"-\"*65+\"\\n\"\n",
    "res += \"\\n\".join(\"{:<10} | {:<10} | {:<10} | {:<13} | {:<10}\".format(x, y, a, b, c) for x, y, a, b, c in zip(y_test, y_pred, probs_y[:,0], probs_y[:,1], probs_y[:,2]))\n",
    "res += \"\\n\"+\"-\"*65+\"\\n\"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of our classifier may not be consistent across classes (for example, certain classes could be easily prediced, but the other constantly misclassified).\n",
    "\n",
    "A confusion matrix allows us to see how distinct each class is from the other.\n",
    "\n",
    "Each row shows the actual class and each column shows the predicted class (the number at intersection of row r and column c shows how often our model predicts a point in class r as belonging to class c). The name comes from the fact that with this matrix it easy to see how often our classifier confuses any two classes.\n",
    "\n",
    "You can use the confusion_matrix() function imported from sklearn.metrics to compute the confusion matrix\n",
    "You can use the heatmap() from the seaborn module to visualise the confusion matrix in the form of a heatmap (matrix of values where the color of each cell depends on the value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "lAyrjfToCFV5",
    "outputId": "e95c47ee-b9c5-4554-e7e4-3f1dea41d71f"
   },
   "outputs": [],
   "source": [
    "#ToDo:  compute the confusion matrix on the test set into variable cm\n",
    "#Insert your own code here \n",
    "\n",
    "print(cm) \n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "\n",
    "#Visualising the confusion matrix\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10}, fmt='d',cmap=\"Blues\", ax = ax )\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on MNIST (20 min, extra task)\n",
    "\n",
    "\n",
    "MNIST (https://www.tensorflow.org/datasets/catalog/mnist) is a handwritten digit recognition dataset, where every image represents a number from 0 to 9 written by hand.\n",
    "\n",
    "It provides an example closer to real life, and presents more of a challenge to classify due to the larger amount of features per data point and the larger number of possible classes.\n",
    "\n",
    "To download the dataset, we will use data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "images, labels = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: split into train and test set\n",
    "\n",
    "#ToDo split the dataset into train and test (into variables images_train, images_test, labels_train, labels_test )\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "plt.imshow(images[0].reshape(28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data exploration\n",
    "\n",
    "To better understand the dataset, we can visualise several images from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the images in the batch, along with the true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx].reshape(28, 28)), cmap='gray')\n",
    "    ax.set_title(labels[idx])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding what kind of images the dataset generally has, we should explore one image in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1].reshape(28, 28))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max() / 2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "        ax.annotate(str(val/255), xy=(y, x), horizontalalignment='center', verticalalignment='center',\n",
    "                    color='white' if img[x][y] < thresh else 'black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset presents more of a challenge, let's repeat the training process from the iris dataset on the newly loaded MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a model to MNIST\n",
    "\n",
    "#ToDo: create and initialise the classifier\n",
    "#Insert your own code here \n",
    "\n",
    "#ToDo: fit the classifier to the train set\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the performance of our classifier. We expect the performance to be worse, as the problem is now more complex.\n",
    "This shows the necessity for more complicated machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on MNIST dataset\n",
    "\n",
    "#ToDo: Predict the Test set results (into y_pred)\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Predict probabilities (into probs_y)\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: round the probabilities up to the 2nd digit after the comma\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Limit the output to only the first 10 elements\n",
    "#Insert your own code here\n",
    "\n",
    "#We plot a table with target results, predicted class and probabilities for each class for every data point\n",
    "res = \"{:<5}| {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5}| {:<5} | {:<5} | {:<5} \".format(\"Label \", \"Pred \", \"0(%)\", \"1(%)\", \"2(%)\", \"3(%)\", \"4(%)\", \"5(%)\", \"6(%)\", \"7(%)\", \"8(%)\", \"9(%)\")\n",
    "res += \"\\n\"+\"-\"*90+\"\\n\"\n",
    "res += \"\\n\".join(\"{:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5} | {:<5}| {:<5} | {:<5} | {:<5} \".format(x, y, a, b, c, d, e, f, g, h, i, j) for x, y, a, b, c, d, e, f, g, h, i, j in zip(labels_test, y_pred, probs_y[:,0], probs_y[:,1], probs_y[:,2], probs_y[:,3], probs_y[:,4], probs_y[:,5], probs_y[:,6], probs_y[:,7], probs_y[:,8], probs_y[:,9]))\n",
    "res += \"\\n\"+\"-\"*90+\"\\n\"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the input and prediction (same as input but predicted with percentages)\n",
    "# Plot the images in the batch, along with the true labels\n",
    "# Plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(np.array(images_test)[idx].reshape(28, 28)), cmap='gray')\n",
    "    ax.set_title(\"{} (true {}), {}%\".format(str(y_pred[idx]), str(labels_test[labels_test.keys()[idx]]), probs_y[idx][int(y_pred[idx])]*100),\n",
    "                 color=(\"green\" if y_pred[idx] == labels_test[labels_test.keys()[idx]] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
