{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609ed97e",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) for Cat/Dog Classification\n",
    "\n",
    "In this notebook, we will build a deep learning Convolutional Neural Network (CNN) model to classify images of cats and dogs. We will use the Pytorch for model creation and training.\n",
    "\n",
    "### Step 1: Import packages for the notebook\n",
    "\n",
    "In this step, we import the necessary packages for our task. The essential packages used in this notebook are:\n",
    "\n",
    "- os: This package provides functions for interacting with the operating system, like file paths and directories.\n",
    "- numpy as np: NumPy is a fundamental package for scientific computing in Python. We use it for numerical operations and array manipulations.\n",
    "- torch: This is the PyTorch library, which is a popular deep learning framework for building and training neural networks.\n",
    "- torch.nn: This module contains all the necessary functions and classes for building neural networks in PyTorch.\n",
    "- torch.optim: This module contains various optimization algorithms used for training neural networks.\n",
    "- torch.utils.data.DataLoader: This class is used to load data efficiently for training and validation.\n",
    "- torchvision.transforms: This module provides common image transformations like resizing, cropping, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import packages for the notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0acfa0",
   "metadata": {},
   "source": [
    "### Step 2: Create a link to the dataset\n",
    "\n",
    "In this step, we define the URL to download the dataset. The dataset contains images of cats and dogs and is hosted on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf782c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a link to the dataset\n",
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Exercise3.03/cats_and_dogs_filtered.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a8ebd",
   "metadata": {},
   "source": [
    "### Step 3: Download the dataset\n",
    "\n",
    "In this step, we download the dataset to the 'data' folder using the provided URL. We use the urllib.request.urlretrieve function to download the zip file and zipfile.ZipFile to extract the contents to the 'data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e59e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download the dataset\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Create a 'data' directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "# Download and extract the dataset to the 'data' directory\n",
    "zip_file_path = 'data/cats_and_dogs.zip'\n",
    "if not os.path.exists(zip_file_path):\n",
    "    urllib.request.urlretrieve(file_url, zip_file_path)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c553537",
   "metadata": {},
   "source": [
    "### Step 4: Create a path variable to cats_and_dogs_filtered directory\n",
    "\n",
    "In this step, we define the path to the main directory where the extracted dataset is located. This will be used to access the training and validation directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create a path variable to cats_and_dogs_filtered directory\n",
    "path = os.path.join('data', 'cats_and_dogs_filtered')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cc809",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5: Create path variables to the train and validation directories\n",
    "\n",
    "In this step, we define variables that hold the paths to the training and validation data directories. These directories contain subdirectories for cats and dogs images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create path variables to the train and validation directories\n",
    "train_dir = os.path.join(path, 'train')\n",
    "validation_dir = os.path.join(path, 'validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb466da",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Create path variables to the other directories\n",
    "\n",
    "In this step, we define additional variables that hold the paths to the subdirectories within the training and validation directories for cats and dogs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b02bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dd0be",
   "metadata": {},
   "source": [
    "\n",
    "### Step 7: Create variables to hold train & validation image counts\n",
    "\n",
    "In this step, we count the total number of images in the training and validation sets. This information will be used later for specifying the number of steps per epoch during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create variables to hold train & validation image counts\n",
    "total_train = len(os.listdir(train_cats_dir)) + len(os.listdir(train_dogs_dir))\n",
    "total_val = len(os.listdir(validation_cats_dir)) + len(os.listdir(validation_dogs_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed6fe4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 8: Assign values to three parameter variables\n",
    "\n",
    "In this step, we set three variables: batch_size, img_height, and img_width. These variables are used to specify the batch size and image dimensions for training and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a000a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Assign values to three parameter variables\n",
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ed9ba",
   "metadata": {},
   "source": [
    "### Step 8: Instantiate two ImageDataGenerator classes\n",
    "\n",
    "In this step, we define two instances of the torchvision.transforms.Compose class. These are used for data preprocessing and data augmentation. The train_transform object contains data augmentation transformations like resizing, normalization, and flipping, while the val_transform object is used for validation data without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Instantiate two ImageDataGenerator classes\n",
    "# Note: The data in these two objects will be rescaled (standardized).\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_width, img_height)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_width, img_height)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff9594",
   "metadata": {},
   "source": [
    "\n",
    "### Step 10: Create the training data generator\n",
    "\n",
    "Here, we create a training data generator using the DataLoader class from PyTorch. This generator will load and preprocess the training data, applying the data augmentation transformations defined in train_transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 10: Create the training data generator\n",
    "# Note that we are using the ImageFolder dataset from PyTorch for convenience, which reads images from the specified directories and applies the given transformations.\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "train_data_gen = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4afaf",
   "metadata": {},
   "source": [
    "\n",
    "### Step 11: Create the validation data generator\n",
    "\n",
    "Similar to the previous step, we create a validation data generator using the DataLoader class. This generator will load and preprocess the validation data using the val_transform transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 11: Create the validation data generator\n",
    "val_dataset = datasets.ImageFolder(root=validation_dir, transform=val_transform)\n",
    "val_data_gen = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cda27",
   "metadata": {},
   "source": [
    "### Show a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca072dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Get a batch of data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Plot the batch of data\n",
    "num_images = len(images)\n",
    "num_cols = 3  # Set the number of columns in the plot\n",
    "\n",
    "# Calculate the number of rows needed to display the images\n",
    "num_rows = int(np.ceil(num_images / num_cols))\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "for i in range(num_images):\n",
    "    image = images[i].permute(1, 2, 0)  # Convert from tensor format (C, H, W) to image format (H, W, C)\n",
    "    image = (image * 0.225) + np.array([0.485, 0.456, 0.406])  # Inverse normalization\n",
    "    image = np.clip(image, 0, 1)  # Clip pixel values to [0, 1] range\n",
    "    label = 'Dog' if labels[i] == 1 else 'Cat'\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    axes[row, col].imshow(image)\n",
    "    axes[row, col].set_title(label)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for i in range(num_images, num_rows*num_cols):\n",
    "    axes.flatten()[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5daf5",
   "metadata": {},
   "source": [
    "\n",
    "### Step 12: Set random seeds for repeatability\n",
    "\n",
    "In deep learning, we often use random processes like weight initialization and data shuffling. To ensure reproducibility of the results, it is essential to set random seeds for random number generators. Here, we set random seeds for NumPy and Torch to have consistent results across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 12: Set random seeds for repeatability\n",
    "np.random.seed(8)\n",
    "torch.manual_seed(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7e4b2",
   "metadata": {},
   "source": [
    "\n",
    "### Step 13: Create our model\n",
    "\n",
    "In this step, we define our convolutional neural network (CNN) model using the nn.Sequential class from PyTorch. The model consists of two convolutional layers with ReLU activation and max-pooling, followed by two fully connected layers with ReLU activation and a final sigmoid activation for binary classification (cat or dog).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Create our model \n",
    "# The model consists of several convolutional layers, followed by max-pooling layers, and fully connected layers for classification.\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        x = self.pool3(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be021a0",
   "metadata": {},
   "source": [
    "\n",
    "### Step 14: Set the optimizer and learning rate\n",
    "\n",
    "In this step, we define the optimizer for the model using the Adam optimizer from torch.optim. The optimizer is responsible for updating the model's parameters during training to minimize the loss. We also specify the learning rate as a hyperparameter for the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc52ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 14: Set the optimizer and learning rate\n",
    "model = CNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebde588",
   "metadata": {},
   "source": [
    "\n",
    "### Step 15: Define the loss function\n",
    "\n",
    "In this step, we define the loss function used for training the model. Since this is a binary classification task, we use Binary Cross Entropy Loss (BCELoss in PyTorch) as the appropriate loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 15: Define the loss function\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ed107",
   "metadata": {},
   "source": [
    "\n",
    "### Step 16: Display the model summary\n",
    "\n",
    "In this step, we display a summary of our CNN model using print(model). This gives us insights into the architecture of the model, the number of parameters in each layer, and the total number of trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 16: Display the model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d02ba",
   "metadata": {},
   "source": [
    "\n",
    "### Step 17: Training Loop\n",
    "\n",
    "Here, we start the training loop. We move the model to the GPU if available and then iterate over the training data for a specified number of epochs. In each epoch, we perform the following steps:\n",
    "\n",
    "Zero the gradients of the model parameters.\n",
    "Perform a forward pass through the model to get the predictions.\n",
    "Calculate the loss between the predictions and the ground truth labels.\n",
    "Perform backpropagation to compute the gradients of the model's parameters.\n",
    "Update the model's parameters using the optimizer.\n",
    "We also calculate the training accuracy at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 17: Training Loop\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.round(outputs).view(-1)\n",
    "        corrects += torch.sum(preds == labels)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = corrects.double() / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9508326",
   "metadata": {},
   "source": [
    "\n",
    "### Step 18: Validation Loop\n",
    "\n",
    "In this step, we evaluate the model's performance on the validation data. We use the validation data generator to iterate over the validation dataset, and for each batch, we calculate the accuracy of the model's predictions against the ground truth labels.\n",
    "\n",
    "Finally, we print the validation accuracy, which gives us an estimate of how well our model generalizes to new, unseen data.\n",
    "\n",
    "With these steps, we have successfully trained a CNN model for classifying cats and dogs using PyTorch. Additionally, we utilized data augmentation to increase the diversity of the training data and improve the model's performance and generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaf052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Validation Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    for inputs, labels in val_data_gen:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs >= 0.5).squeeze().long()\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "        val_samples += labels.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b295b",
   "metadata": {},
   "source": [
    "## Exercise: Improving the Model with Data Augmentation\n",
    "\n",
    "In the current implementation, we trained the CNN model using a limited amount of data, which could lead to overfitting. To address this limitation and improve the model's performance, we can utilize data augmentation techniques to generate additional training images. Data augmentation involves applying random transformations, such as rotation, flipping, zooming, and shifting, to the original images, creating variations of the training data.\n",
    "\n",
    "In this exercise, you will implement data augmentation using the torchvision.transforms module from PyTorch. Your task is to modify the data preprocessing and augmentation steps (Step 8) and the training and validation data generators (Steps 10 and 11) to include data augmentation.\n",
    "\n",
    "Follow these steps to implement data augmentation:\n",
    "\n",
    "- Step 1: Update Data Preprocessing and Augmentation\n",
    "\n",
    "Import the required modules from torchvision.transforms.\n",
    "Modify the train_transform object to include data augmentation techniques, such as random rotation, horizontal flipping, and random resized crop.\n",
    "Keep the val_transform object as it is since we don't perform data augmentation on the validation data.\n",
    "- Step 2: Update Training Data Generator\n",
    "\n",
    "Replace the current train_data_gen with a new data generator using the modified train_transform.\n",
    "Adjust the batch size and other parameters as needed.\n",
    "- Step 3: Run the Model Training\n",
    "\n",
    "Re-run the training loop with the updated data generator.\n",
    "Observe how data augmentation impacts the model's training process and its accuracy on the training and validation datasets.\n",
    "- Step 4: Experiment with Different Augmentation Techniques\n",
    "\n",
    "To further enhance the model's performance, try different combinations of data augmentation techniques and observe their effects on the model's accuracy and generalization.\n",
    "Experiment with additional augmentation options like random vertical flipping, color jittering, or brightness adjustments.\n",
    "By implementing data augmentation and experimenting with different augmentation techniques, you can improve your CNN model's performance and achieve better accuracy on the validation dataset. Keep in mind that the choice of augmentation techniques depends on the specific problem and dataset characteristics, so it's essential to find the right balance between data diversity and model generalization. Happy experimenting!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e935d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(8)\n",
    "torch.manual_seed(8)\n",
    "\n",
    "# Define the data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomResizedCrop(100),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize((100, 100)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Set the data directories\n",
    "data_dir = 'cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "validation_dir = os.path.join(data_dir, 'validation')\n",
    "\n",
    "# Create dataset objects using torchvision.datasets.ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(validation_dir, transform=data_transforms['validation'])\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the CNN model\n",
    "class CustomCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        x = self.pool3(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Create the model and set the optimizer\n",
    "model = CustomCNNModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reduced learning rate for faster convergence\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = torch.round(outputs).view(-1)\n",
    "        corrects += torch.sum(preds == labels)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_accuracy = corrects.double() / len(train_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "val_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.round(outputs).view(-1)\n",
    "        val_corrects += torch.sum(preds == labels)\n",
    "\n",
    "val_accuracy = val_corrects.double() / len(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
